# -*- coding: utf-8 -*-
"""BERT-based NLP program-DL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1INw2KMTAq5torL2UcKV34XU1axcHEpLV

Week 1: NLP Fundamentals & Text Preprocessing
Topics:

NLP pipeline overview

Text cleaning (lowercasing, removing special chars)

Tokenization (word, subword, character-level)

Stopword removal, stemming, lemmatization

Vocabulary building

Text normalization techniques
"""

import nltk

# Download all NLTK data (large download ~500MB)
nltk.download('all')

# Now the tokenization will work
from nltk.tokenize import word_tokenize
text = "Deep learning models are achieving state-of-the-art results in NLP."
print(word_tokenize(text.lower()))

import nltk

# Download only the necessary resources
nltk.download('punkt')  # For tokenization
nltk.download('wordnet')  # For lemmatization
nltk.download('omw-1.4')  # Open Multilingual WordNet (required for lemmatization)

# If you still get punkt_tab error, try:
nltk.download('punkt_tab')

# Now your code will work
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer

text = "Deep learning models are achieving state-of-the-art results in NLP."

# Tokenization
tokens = word_tokenize(text.lower())
print("Tokens:", tokens)

# Stemming
stemmer = PorterStemmer()
stems = [stemmer.stem(token) for token in tokens]
print("Stems:", stems)

# Lemmatization
lemmatizer = WordNetLemmatizer()
lemmas = [lemmatizer.lemmatize(token) for token in tokens]
print("Lemmas:", lemmas)

pip install gensim

"""Week 2: Word Embeddings & Text Representation
Topics:

Bag-of-Words (BoW) and TF-IDF

Word2Vec (Skip-gram, CBOW)

GloVe embeddings

FastText

Embedding visualization (t-SNE, PCA)


"""

from gensim.models import Word2Vec
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

sentences = [["deep", "learning", "is", "powerful"],
             ["nlp", "uses", "deep", "learning"],
             ["word", "embeddings", "are", "important"]]

# Train Word2Vec model
model = Word2Vec(sentences, vector_size=50, window=3, min_count=1, workers=4)

# Visualize embeddings
words = list(model.wv.key_to_index)
vectors = [model.wv[word] for word in words]

pca = PCA(n_components=2)
result = pca.fit_transform(vectors)

plt.scatter(result[:, 0], result[:, 1])
for i, word in enumerate(words):
    plt.annotate(word, xy=(result[i, 0], result[i, 1]))
plt.show()

"""Week 3: Sequence Models & RNNs
Topics:

Recurrent Neural Networks (RNNs)

Long Short-Term Memory (LSTM) networks

Gated Recurrent Units (GRUs)

Bidirectional RNNs

Sequence-to-sequence models

Hands-on (PyTorch example):
"""

import torch
import torch.nn as nn

class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, text):
        embedded = self.embedding(text)
        output, (hidden, cell) = self.lstm(embedded)
        return self.fc(hidden.squeeze(0))

# Example usage
model = LSTMClassifier(vocab_size=10000, embedding_dim=100,
                      hidden_dim=256, output_dim=2)

"""Week 4: Attention Mechanisms & Transformers
Topics:

Attention mechanism fundamentals

Transformer architecture

Self-attention

Multi-head attention

Positional encoding

Hands-on (Implementing attention):
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class Attention(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.attention = nn.Linear(hidden_dim, 1)

    def forward(self, encoder_outputs):
        # encoder_outputs: [batch_size, seq_len, hidden_dim]
        attention_scores = self.attention(encoder_outputs).squeeze(2)
        attention_weights = F.softmax(attention_scores, dim=1)
        context_vector = torch.bmm(attention_weights.unsqueeze(1),
                                 encoder_outputs).squeeze(1)
        return context_vector, attention_weights

"""Week 5: Pretrained Language Models & Transfer Learning
Topics:

BERT architecture

GPT models

Fine-tuning pretrained models

Hugging Face Transformers library

Model distillation

Hands-on (BERT fine-tuning):
"""

from transformers import BertTokenizer, BertForSequenceClassification
import torch.optim as optim  # Import from PyTorch instead

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased',
                                                   num_labels=2)

# Use PyTorch's AdamW
optimizer = optim.AdamW(model.parameters(), lr=5e-5)
inputs = tokenizer("This is a sample text for classification",
                  return_tensors="pt",
                  padding=True,
                  truncation=True)

# Forward pass
outputs = model(**inputs)

from transformers import logging
logging.set_verbosity_error()  # Suppresses warnings

# Your existing code here
outputs = model(**inputs)
print("Logits:", outputs.logits)  # Force display the actual output

# Add this at the end to force display
print("\n=== MODEL OUTPUT ===")
print("Logits shape:", outputs.logits.shape)
print("Sample prediction:", outputs.logits)

