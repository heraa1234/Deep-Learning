# -*- coding: utf-8 -*-
"""Sentimental Analysis with LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gHlkLPAfFIxWA7eqwv-b26dzPwFmDrYu
"""

# Import necessary libraries
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
import numpy as np

# Check TensorFlow version
print("TensorFlow version:", tf.__version__)

# 1. Load and preprocess the IMDB dataset
max_features = 10000  # Number of words to consider (most frequent)
maxlen = 200  # Maximum length of each review (in words)

try:
    # Load IMDB dataset
    (x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)
    print("Dataset loaded successfully.")
    print("x_train shape:", x_train.shape, "y_train shape:", y_train.shape)
    print("x_test shape:", x_test.shape, "y_test shape:", y_test.shape)
except Exception as e:
    print("Error loading dataset:", e)
    raise

# Pad sequences to ensure uniform input length
x_train = pad_sequences(x_train, maxlen=maxlen)
x_test = pad_sequences(x_test, maxlen=maxlen)
print("After padding, x_train shape:", x_train.shape, "x_test shape:", x_test.shape)

# Verify data integrity
if x_train.size == 0 or y_train.size == 0:
    raise ValueError("Training data is empty. Check dataset loading.")
if not np.all(np.isin(y_train, [0, 1])) or not np.all(np.isin(y_test, [0, 1])):
    raise ValueError("Labels contain values other than 0 or 1.")

# 2. Build the LSTM model
model = Sequential()
model.add(Embedding(input_dim=max_features, output_dim=128, input_length=maxlen))
model.add(LSTM(units=64, return_sequences=False))
model.add(Dense(units=1, activation='sigmoid'))

# 3. Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# 4. Print model summary
model.summary()

# 5. Convert data to TensorFlow Dataset to avoid iterator issues
train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))
train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)

# 6. Train the model using the dataset
try:
    model.fit(train_dataset, epochs=3, validation_split=0.2)
except Exception as e:
    print("Error during training:", e)
    # Fallback: Train without dataset API
    print("Trying fallback training method...")
    model.fit(x_train, y_train, batch_size=32, epochs=3, validation_split=0.2)

# 7. Evaluate the model
test_loss, test_accuracy = model.evaluate(x_test, y_test)
print(f'\nTest Accuracy: {test_accuracy:.4f}')

# 8. Make a prediction on a sample review
sample_review = x_test[0]
prediction = model.predict(np.expand_dims(sample_review, axis=0))
print(f'\nSample Review Prediction: {"Positive" if prediction[0] > 0.5 else "Negative"}')

